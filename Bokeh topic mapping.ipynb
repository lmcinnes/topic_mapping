{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling via Clustering Embeddings\n",
    "\n",
    "This is a very code heavy notebook about topic modelling and visualization thereof. Mostly visualization thereof. We will be making use of the vectorizer library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets\n",
    "import sklearn.preprocessing\n",
    "import scipy.sparse\n",
    "import vectorizers\n",
    "import vectorizers.transformers\n",
    "import umap\n",
    "import umap.plot\n",
    "import pynndescent\n",
    "import seaborn as sns\n",
    "import matplotlib.colors\n",
    "import hdbscan\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step, our test data -- the standard 20-nbewsgroups dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = sklearn.datasets.fetch_20newsgroups(\n",
    "    subset=\"all\", remove=(\"headers\", \"footers\", \"quotes\")\n",
    ")\n",
    "long_enough = [len(t) > 200 for t in news[\"data\"]]\n",
    "targets = np.array(news.target)[long_enough]\n",
    "news_data = [t for t in news[\"data\"] if len(t) > 200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll do very simple tokenization; good enough to get the job done. In a more advanced version of this we might use something like sentence-piece instead to learn a tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1 s, sys: 50.1 ms, total: 1.05 s\n",
      "Wall time: 1.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cv = sklearn.feature_extraction.text.CountVectorizer(lowercase=True)\n",
    "sk_word_tokenize = cv.build_tokenizer()\n",
    "sk_preprocesser = cv.build_preprocessor()\n",
    "tokenize = lambda doc: sk_word_tokenize(sk_preprocesser(doc))\n",
    "tokenized_news = [tokenize(doc) for doc in news_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we'll use the ``TokenCooccurrenceVectorizer`` to generate word vectors learned directly from the corpus. This has the benefit that we learn idiomatic word usage. It has the downside that we have an issue when we don't have enough text to learn good word vectors. In an ideal world we could use some pretrained material to manage to make this more tractable -- and indeed we can use a Bayesian prior (pre-trained on a larger corpus) on the co-occurrence matrix, but that isn't implemented yet. Fortunaetly, despite being quite a small datasets, 20 newsgroups is \"big enough\" to learn reasonable word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 58s, sys: 3.69 s, total: 2min 1s\n",
      "Wall time: 1min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "word_vectorizer = vectorizers.TokenCooccurrenceVectorizer(\n",
    "    min_document_occurrences=5,\n",
    "    window_radii=10,          \n",
    "    window_functions='variable',\n",
    "    kernel_functions='geometric',            \n",
    "    n_iter = 0,\n",
    "    normalize_windows=True,\n",
    ").fit(tokenized_news)\n",
    "word_vectors = word_vectorizer.reduce_dimension(dimension=160, algorithm=\"randomized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need document embeddings, and, to power the topic modelling, word vectors that live in the same space as the topic vectors. Fortunately this is actually surprisingly easy to arrange -- we create a document matrix of word vectors (i.e. the identity matrix) and just push that through the same pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.2 s, sys: 387 ms, total: 20.6 s\n",
      "Wall time: 17.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc_matrix = vectorizers.NgramVectorizer(\n",
    "    token_dictionary=word_vectorizer.token_label_dictionary_\n",
    ").fit_transform(tokenized_news)\n",
    "info_transformer = vectorizers.transformers.InformationWeightTransformer(\n",
    "    prior_strength=1e-1,\n",
    "    approx_prior=False,\n",
    ")\n",
    "info_doc_matrix = info_transformer.fit_transform(doc_matrix)\n",
    "lat_vectorizer = vectorizers.ApproximateWassersteinVectorizer(\n",
    "    normalization_power=0.66,\n",
    "    random_state=42,\n",
    ")\n",
    "lat_doc_vectors = lat_vectorizer.fit_transform(info_doc_matrix, vectors=word_vectors)\n",
    "lat_word_vectors = lat_vectorizer.transform(info_transformer.transform(scipy.sparse.eye(word_vectors.shape[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can do some topic modelling -- our goal is to cluster a low dimensional representation of the document evctors and consider each cluster a \"topic\". We can then generate \"topic words\" for each topic by taking the closest words to the cluster centroid in the joint document-word vector space we just created. This is essentially the same as what Top2Vec does, but we aren't using doc2vec, and we'll be directly using the HDBSCAN cluster hierarchy for varied granularity of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_cluster_tree(doc_vectors, min_cluster_size=50):\n",
    "    low_dim_rep = umap.UMAP(\n",
    "        metric=\"cosine\", n_components=5, min_dist=1e-4, random_state=42, n_epochs=500\n",
    "    ).fit_transform(doc_vectors)\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size).fit(low_dim_rep)\n",
    "    tree = clusterer.condensed_tree_.to_pandas()\n",
    "    return tree\n",
    "\n",
    "def get_points(tree, cluster_id):\n",
    "    child_rows = tree[tree.parent == cluster_id]\n",
    "    result_points = []\n",
    "    result_lambdas = []\n",
    "    for i, row in child_rows.iterrows():\n",
    "        if row.child_size == 1:\n",
    "            result_points.append(int(row.child))\n",
    "            result_lambdas.append(row.lambda_val)\n",
    "        else:\n",
    "            points, lambdas = get_points(tree, row.child)\n",
    "            result_points.extend(points)\n",
    "            result_lambdas.extend(lambdas)\n",
    "    return result_points, result_lambdas\n",
    "\n",
    "def get_topic_words(tree, cluster_id, vectors, nn_index, index_to_word_fn):\n",
    "    row_ids, weights = get_points(tree, cluster_id)\n",
    "    centroid = np.mean(vectors[row_ids], axis=0)\n",
    "    if pynndescent.distances.cosine(centroid, np.mean(vectors, axis=0)) < 0.2:\n",
    "        dists, inds = nn_index.kneighbors([centroid])\n",
    "        return [\"☼Generic☼\"], [np.mean(dists)], len(row_ids)\n",
    "    dists, inds = nn_index.kneighbors([centroid])\n",
    "    keywords = [index_to_word_fn(x) for x in inds[0]]\n",
    "    return keywords, dists[0], len(row_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how do we visualize all of this? We can generate topic words for any cluster, and we can also generate a low dimensional representation of the documents, allowing us the place clusters in a 2D plot. The trick now is to represent the cluster with it's topic words is a somewhat space filling way. The trick to doing that is to make use of word clouds -- specifically the wordcloud package which allows the use of shaped masks in word cloud generation. Our goal will be to generate a shape mask based on the cluster and then use the wordcloud package to pack the topic words into that shape. We can then overlay all these word clouds according to the 2D layout and hopefully produce a useful visualization. This is a surprisingly large amount of work (because there are a lot of parts here). We'll start with a bucket load of imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.cluster\n",
    "from sklearn.neighbors import NearestNeighbors, KernelDensity\n",
    "from matplotlib.colors import rgb2hex, Normalize\n",
    "from skimage.transform import rescale\n",
    "import wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need a lot of helper functions. Most of this is simply cobbled together as the simplest coding solution (rather than the most efficient implementation) to extract relevant data. One thing we will need, to make visualizing the hierarchy at least somewhat tractable, is the ability to slice it into layers. Since we'll need both the clusters at a given slice-layer, and the epsilon values (for better estimating kernel bandwidths per cluster later on) we'll need functions for each. We'll also have to be able to convert those cluster selections into actual label vectors as one might expect to get out of a ``fit_predict``. None of this is that hard, just a little tedious -- it helps to be somewhat familiar with some of the inner workings of HDBSCAN to make this easier to parse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusters_at_level(tree_df, level):\n",
    "    clusters = tree_df[tree_df.child_size > 1]\n",
    "    cluster_born_before_level = clusters.child[clusters.lambda_val <= level]\n",
    "    cluster_dies_after_level = clusters.parent[clusters.lambda_val > level]\n",
    "    cluster_is_leaf = np.setdiff1d(clusters.child, clusters.parent)\n",
    "    clusters_lives_after_level = np.union1d(cluster_dies_after_level, cluster_is_leaf)\n",
    "    result = np.intersect1d(cluster_born_before_level, clusters_lives_after_level)\n",
    "    return result\n",
    "\n",
    "def clusters_eps_at_level(tree_df, level):\n",
    "    clusters = tree_df[tree_df.child_size > 1]\n",
    "    cluster_born_before_level = clusters.child[clusters.lambda_val <= level]\n",
    "    cluster_dies_after_level = clusters.parent[clusters.lambda_val > level]\n",
    "    cluster_is_leaf = np.setdiff1d(clusters.child, clusters.parent)\n",
    "    clusters_lives_after_level = np.union1d(cluster_dies_after_level, cluster_is_leaf)\n",
    "    chosen_clusters = np.intersect1d(\n",
    "        cluster_born_before_level, clusters_lives_after_level\n",
    "    )\n",
    "    result = [\n",
    "        (1.0 / clusters.lambda_val[clusters.child == cid].values[0])\n",
    "        for cid in chosen_clusters\n",
    "    ]\n",
    "    return result\n",
    "\n",
    "def create_labels(tree, cluster_ids, n_points):\n",
    "    result = np.full(n_points, -1)\n",
    "    for i, cluster_id in enumerate(cluster_ids):\n",
    "        point_ids, _ = get_points(tree, cluster_id)\n",
    "        result[point_ids] = i\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next problem is colours. Colours are hard to get right. We want colours to be meaningful over different layers. The easist way to do that is to assign a colour scheme to the leaves of the cluster tree in some vaguely consistent way, and then average colours together to get colours for clusters in higher layers. Since we will have a *lot* of leaf clusters we'll need a huge palette. As a hack to ensure that the colour averaging doesn't produce nothing but muddy browns we can use some thematic colour belnds and ensure they are well placed with reagrd to our 2D layout of the leaf clusters. Since I managed 4 different colour blends we can use KMeans to cluster the leaf clusters in 2D and then just assign colours from the blend palettes within clusters. This is perhaps more complicated than it needs to be but it makes the aesthetics of the final plots a lot nicer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_colors(hex_colors, weights):\n",
    "    rgb_colors = matplotlib.colors.to_rgba_array(hex_colors)\n",
    "    result_color = np.sqrt(np.average(rgb_colors ** 2, weights=weights, axis=0))\n",
    "    return matplotlib.colors.to_hex(result_color[:3])\n",
    "\n",
    "\n",
    "def create_leaf_color_key(clusterer, doc_vectors):\n",
    "    n_leaves = np.max(clusterer.labels_) + 1\n",
    "    embedding_rep = umap.UMAP(metric=\"cosine\", random_state=42).fit_transform(\n",
    "        doc_vectors\n",
    "    )\n",
    "    embedding_leaf_centroids = np.array(\n",
    "        [np.mean(embedding_rep[clusterer.labels_ == i], axis=0) for i in range(n_leaves)]\n",
    "    )\n",
    "    leaf_nbrs = NearestNeighbors(n_neighbors=3, metric=\"euclidean\").fit(\n",
    "        embedding_leaf_centroids\n",
    "    )\n",
    "    kmeans_classes = sklearn.cluster.KMeans(n_clusters=4).fit_predict(\n",
    "        embedding_leaf_centroids\n",
    "    )\n",
    "    km_based_labelling = np.zeros(embedding_leaf_centroids.shape[0], dtype=np.int64)\n",
    "    km_based_labelling[kmeans_classes == 0] = np.arange(np.sum(kmeans_classes == 0))\n",
    "    km_based_labelling[kmeans_classes == 1] = (\n",
    "        np.arange(np.sum(kmeans_classes == 1)) + np.max(km_based_labelling) + 1\n",
    "    )\n",
    "    km_based_labelling[kmeans_classes == 2] = (\n",
    "        np.arange(np.sum(kmeans_classes == 2)) + np.max(km_based_labelling) + 1\n",
    "    )\n",
    "    km_based_labelling[kmeans_classes == 3] = (\n",
    "        np.arange(np.sum(kmeans_classes == 3)) + np.max(km_based_labelling) + 1\n",
    "    )\n",
    "    cluster_order = dict(np.vstack([np.arange(n_leaves), km_based_labelling]).T)\n",
    "    cluster_leaves = np.array(\n",
    "        [cluster_order[x] if x >= 0 else -1 for x in clusterer.labels_]\n",
    "    )\n",
    "    color_key = (\n",
    "        list(\n",
    "            sns.blend_palette(\n",
    "                [\"#fbbabd\", \"#a566cc\", \"#51228d\"], np.sum(kmeans_classes == 0)\n",
    "            ).as_hex()\n",
    "        )\n",
    "        + list(\n",
    "            sns.blend_palette(\n",
    "                [\"#ffefa0\", \"#fd7034\", \"#9d0d14\"], np.sum(kmeans_classes == 1)\n",
    "            ).as_hex()\n",
    "        )\n",
    "        + list(\n",
    "            sns.blend_palette(\n",
    "                [\"#a0f0d0\", \"#4093bf\", \"#084d96\"], np.sum(kmeans_classes == 2)\n",
    "            ).as_hex()\n",
    "        )\n",
    "        + list(\n",
    "            sns.blend_palette(\n",
    "                [\"#e0f3a4\", \"#66cc66\", \"#006435\"], np.sum(kmeans_classes == 3)\n",
    "            ).as_hex()\n",
    "        )\n",
    "    )\n",
    "    return color_key, cluster_order, leaf_nbrs, embedding_rep\n",
    "\n",
    "def create_cluster_layer_color_key(tree, layer, embedding_rep, leaf_nbrs, leaf_color_key, leaf_dict):\n",
    "    cluster_labels = create_labels(tree, layer, embedding_rep.shape[0])\n",
    "    cluster_centroids = np.array(\n",
    "        [\n",
    "            np.mean(embedding_rep[cluster_labels == i], axis=0)\n",
    "            for i in range(np.max(cluster_labels) + 1)\n",
    "        ]\n",
    "    )\n",
    "    leaf_dists, leaf_inds = leaf_nbrs.kneighbors(cluster_centroids)\n",
    "    leaf_dists += np.finfo(np.float32).eps\n",
    "    color_key = [\n",
    "        avg_colors(\n",
    "            [leaf_color_key[leaf_dict[x]] for x in leaves],\n",
    "            np.nan_to_num(1.0 / (leaf_dists[i])),\n",
    "        )\n",
    "        for i, leaves in enumerate(leaf_inds)\n",
    "    ]\n",
    "    return cluster_labels, color_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for plotting. To make this useful it really needs to be interactive. I experimented with a few options for this, but Bokeh was the easiest for me to get quick results. Ideally PyDeck would do a good job of this, but I struggled to get the wordclouds working well with PyDeck -- likely due to my lack of expertise in deck.gl. So, bokeh it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.io import curdoc, show, output_notebook, output_file\n",
    "from bokeh.models import (\n",
    "    ColumnDataSource,\n",
    "    Grid,\n",
    "    LinearAxis,\n",
    "    Plot,\n",
    "    Text,\n",
    "    CustomJS,\n",
    "    ImageRGBA,\n",
    "    Range1d,\n",
    "    Slider,\n",
    "    DataTable,\n",
    "    TableColumn,\n",
    "    HTMLTemplateFormatter,\n",
    "    Div,\n",
    "    LassoSelectTool,\n",
    "    TapTool,\n",
    "    BoxSelectTool,\n",
    "    Button,\n",
    ")\n",
    "from bokeh.plotting import figure, Figure\n",
    "from bokeh.layouts import column, row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need some plotting helper functions. First something to generate the word cloud and populate data for a bokeh ColumnDataSource with the relevant information. We'll also need to be able to generate a KDE for each cluster and from that generate: a mask for the word cloud; and a \"glow\" effect based on the KDE. The latter is handled by a bokeh ``ImageRGBA`` class, but could equally well be handled by a ``contourf`` style effect in matplotlib or a ``Heatmap`` in PyDeck if we were using those instead.\n",
    "\n",
    "Lastly we have a giant function to handle generating all the data and plot pieces for a single cluster layer -- the word clouds, the ``ImageRGBA`` for a glow effect, and the appropriately coloured scatterplot of the individual documents. We also need a custom javascript callback so that the text size in the wordcloud scales with the zoom so that we can \"zoom in\" and see the smaller words in the word clouds.\n",
    "\n",
    "**Note**: the glow effect has been disabled in this version as it significantly bloats the resulting HTML output (it costs a lot of memory to store all that data). It works fine locally, but is poor for putting on a remote sight. The code has been left in (but commented out) so it can be re-enabled easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_word_cloud(column_data, word_cloud, size, extent, color, font_scaling=0.66):\n",
    "    raw_height = size[0]\n",
    "    raw_width = size[1]\n",
    "    height = extent[3] - extent[2]\n",
    "    width = extent[1] - extent[0]\n",
    "    x_scaling = width / raw_width\n",
    "    y_scaling = height / raw_height\n",
    "    max_scaling = max(x_scaling, y_scaling)\n",
    "    for row in word_cloud.layout_:\n",
    "        column_data[\"x\"].append(row[2][1] * x_scaling + extent[0])\n",
    "        column_data[\"y\"].append((raw_height - row[2][0]) * y_scaling + extent[2])\n",
    "        column_data[\"text\"].append(row[0][0])\n",
    "        column_data[\"angle\"].append(np.pi / 2 if row[3] is not None else 0.0)\n",
    "        column_data[\"align\"].append(\"right\" if row[3] is not None else \"left\")\n",
    "        column_data[\"baseline\"].append(\"top\" if row[3] is not None else \"top\")\n",
    "        column_data[\"color\"].append(color)\n",
    "        column_data[\"base_size\"].append(f\"{(row[1] * font_scaling) * max_scaling}px\")\n",
    "        column_data[\"current_size\"].append(f\"{(row[1] * font_scaling) * max_scaling}px\")\n",
    "    return column_data\n",
    "\n",
    "\n",
    "def kde_for_cluster(\n",
    "    cluster_embedding, approx_patch_size, cluster_epsilon, kernel_bandwidth_multiplier, color\n",
    "):\n",
    "    kernel_bandwidth = min(\n",
    "        kernel_bandwidth_multiplier * np.power(cluster_epsilon, 0.75),\n",
    "        kernel_bandwidth_multiplier,\n",
    "    )\n",
    "    xmin, xmax = (\n",
    "        np.min(cluster_embedding.T[0]) - 8 * kernel_bandwidth,\n",
    "        np.max(cluster_embedding.T[0]) + 8 * kernel_bandwidth,\n",
    "    )\n",
    "    ymin, ymax = (\n",
    "        np.min(cluster_embedding.T[1]) - 8 * kernel_bandwidth,\n",
    "        np.max(cluster_embedding.T[1]) + 8 * kernel_bandwidth,\n",
    "    )\n",
    "    width = xmax - xmin\n",
    "    height = ymax - ymin\n",
    "    aspect_ratio = width / height\n",
    "    patch_size = min(\n",
    "        max(max(width, height) * approx_patch_size / 6.0, approx_patch_size), 256\n",
    "    )\n",
    "    patch_width = int(patch_size * aspect_ratio)\n",
    "    patch_height = int(patch_size)\n",
    "    xs = np.linspace(xmin, xmax, patch_width)\n",
    "    ys = np.linspace(ymin, ymax, patch_height)\n",
    "    xv, yv = np.meshgrid(xs, ys[::-1])\n",
    "    for_scoring = np.vstack([xv.ravel(), yv.ravel()]).T\n",
    "\n",
    "    cluster_kde = KernelDensity(bandwidth=kernel_bandwidth, kernel=\"gaussian\").fit(\n",
    "        cluster_embedding\n",
    "    )\n",
    "    base_zv = cluster_kde.score_samples(for_scoring).reshape(xv.shape)\n",
    "    zv = rescale(base_zv, 4)\n",
    "    mask = (np.exp(zv) < 2e-2) * 0xFF\n",
    "\n",
    "    img = np.empty((zv.shape[0], zv.shape[1]), dtype=np.uint32)\n",
    "    view = img.view(dtype=np.uint8).reshape((zv.shape[0], zv.shape[1], 4))\n",
    "    view[:, :, :] = (\n",
    "        255\n",
    "        * np.tile(\n",
    "            matplotlib.colors.to_rgba(color), (zv.shape[0], zv.shape[1], 1),\n",
    "        )\n",
    "    ).astype(np.uint8)\n",
    "    view[:, :, 3] = np.round(128 * np.exp(zv - np.max(zv))).astype(np.uint8)\n",
    "\n",
    "    return mask, img, (xmin, xmax, ymin, ymax)\n",
    "\n",
    "\n",
    "def topic_word_by_cluster_layer(\n",
    "    plot,\n",
    "    layer_index,\n",
    "    doc_vectors,\n",
    "    word_vectors,\n",
    "    cluster_labels,\n",
    "    cluster_epsilons,\n",
    "    umap_embedding,\n",
    "    index_to_word_fn,\n",
    "    color_key,\n",
    "    n_neighbors=150,\n",
    "    kernel_bandwidth_multiplier=0.2,\n",
    "    approx_patch_size=64,\n",
    "):\n",
    "    unique_clusters = np.unique(cluster_labels)\n",
    "    unique_clusters = unique_clusters[unique_clusters >= 0]\n",
    "    word_nbrs = NearestNeighbors(n_neighbors=n_neighbors, metric=\"cosine\").fit(\n",
    "        word_vectors\n",
    "    )\n",
    "    cluster_centroids = [\n",
    "        np.mean(doc_vectors[cluster_labels == label], axis=0)\n",
    "        for label in unique_clusters\n",
    "    ]\n",
    "    topic_word_dists, topic_word_indices = word_nbrs.kneighbors(cluster_centroids)\n",
    "\n",
    "    word_cloud_source = dict(\n",
    "        x=[],\n",
    "        y=[],\n",
    "        text=[],\n",
    "        angle=[],\n",
    "        align=[],\n",
    "        baseline=[],\n",
    "        color=[],\n",
    "        base_size=[],\n",
    "        current_size=[],\n",
    "    )\n",
    "\n",
    "    img_source = dict(image=[], x=[], y=[], dh=[], dw=[])\n",
    "\n",
    "    for i, label in enumerate(unique_clusters):\n",
    "        topic_words_and_freqs = {\n",
    "            index_to_word_fn(idx): (1.0 - topic_word_dists[i, j])\n",
    "            for j, idx in enumerate(topic_word_indices[i])\n",
    "        }\n",
    "\n",
    "        cluster_embedding = umap_embedding[cluster_labels == label]\n",
    "\n",
    "        mask, img, extent = kde_for_cluster(\n",
    "            cluster_embedding,\n",
    "            approx_patch_size,\n",
    "            cluster_epsilons[i],\n",
    "            kernel_bandwidth_multiplier,\n",
    "            color_key[label],\n",
    "        )\n",
    "\n",
    "        img_source[\"image\"].append(img[::-1])\n",
    "        img_source[\"x\"].append(extent[0])\n",
    "        img_source[\"y\"].append(extent[2])\n",
    "        img_source[\"dw\"].append(extent[1] - extent[0])\n",
    "        img_source[\"dh\"].append(extent[3] - extent[2])\n",
    "\n",
    "        color_func = lambda *args, **kwargs: color_key[label]\n",
    "        wc = wordcloud.WordCloud(\n",
    "            font_path=\"/home/leland/.fonts/consola.ttf\",\n",
    "            mode=\"RGBA\",\n",
    "            relative_scaling=1,\n",
    "            min_font_size=1,\n",
    "            max_font_size=128,\n",
    "            background_color=None,\n",
    "            color_func=color_func,\n",
    "            mask=mask,\n",
    "        )\n",
    "        wc.fit_words(topic_words_and_freqs)\n",
    "        word_cloud_source = add_word_cloud(\n",
    "            word_cloud_source,\n",
    "            wc,\n",
    "            color=color_key[label],\n",
    "            size=img.shape[:2],\n",
    "            extent=extent,\n",
    "        )\n",
    "    scatter_source = ColumnDataSource(\n",
    "        dict(\n",
    "            x=umap_embedding.T[0],\n",
    "            y=umap_embedding.T[1],\n",
    "            color=[\n",
    "                color_key[label] if label >= 0 else \"#aaaaaa\"\n",
    "                for label in cluster_labels\n",
    "            ],\n",
    "        )\n",
    "    )\n",
    "#     img_source = ColumnDataSource(img_source)\n",
    "    word_cloud_source = ColumnDataSource(word_cloud_source)\n",
    "    scatter_renderer = plot.circle(\n",
    "        x=\"x\",\n",
    "        y=\"y\",\n",
    "        color=\"color\",\n",
    "        radius=2e-2,\n",
    "        alpha=0.25,\n",
    "        line_alpha=0.0,\n",
    "        level=\"glyph\",\n",
    "        source=scatter_source,\n",
    "        tags=[f\"layer{layer_idx}\"],\n",
    "        selection_alpha=1.0,\n",
    "    )\n",
    "#     image_renderer = plot.image_rgba(\n",
    "#         image=\"image\",\n",
    "#         x=\"x\",\n",
    "#         y=\"y\",\n",
    "#         dw=\"dw\",\n",
    "#         dh=\"dh\",\n",
    "#         source=img_source,\n",
    "#         level=\"underlay\",\n",
    "#         tags=[f\"layer{layer_idx}\"],\n",
    "#     )\n",
    "    glyph = Text(\n",
    "        x=\"x\",\n",
    "        y=\"y\",\n",
    "        text=\"text\",\n",
    "        angle=\"angle\",\n",
    "        text_color=\"color\",\n",
    "        text_font={\"value\": \"Consolas\"},\n",
    "        text_font_size=\"current_size\",\n",
    "        text_align=\"align\",\n",
    "        text_baseline=\"baseline\",\n",
    "        text_line_height=1.0,\n",
    "        tags=[f\"layer{layer_idx}\"],\n",
    "    )\n",
    "    text_renderer = plot.add_glyph(word_cloud_source, glyph, level=\"glyph\")\n",
    "    text_callback = CustomJS(\n",
    "        args=dict(source=word_cloud_source),\n",
    "        code=\"\"\"\n",
    "        var data = source.data;\n",
    "        var scale = 1.0 / ((cb_obj.end - cb_obj.start) / 800);\n",
    "        var base_size = data['base_size'];\n",
    "        var current_size = data['current_size'];\n",
    "        for (var i = 0; i < base_size.length; i++) {\n",
    "            current_size[i] = (scale * parseFloat(base_size[i])) + \"px\";\n",
    "        }\n",
    "        source.change.emit();\n",
    "    \"\"\",\n",
    "    )\n",
    "    plot.x_range.js_on_change(\"start\", text_callback)\n",
    "    plot.lod_threshold = None\n",
    "    plot.background_fill_color = \"black\"\n",
    "    plot.axis.ticker = []\n",
    "    plot.grid.grid_line_color = None\n",
    "\n",
    "#     return [text_renderer, scatter_renderer, img_renderer], scatter_source\n",
    "    return [text_renderer, scatter_renderer], scatter_source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now we are ready. We set an output for the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file(\"bokeh_20newsgroups_topics_map_20210526_compressed.html\", title=\"Topic Map of 20 Newsgroups\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here it is a matter of just building the final plot layer by layer. We can add a slider to allow interactively moving through layers, a way to visualize selected posts in an HTML div, and, while we are at it, a download button to download the contents of selected posts. This ends up being a lot of code, but much of it is plotting boilerplate and setting up all the various interactions so that they are all handled with javascript callbacks making the final html output entirely self-contained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "doc_vectors = lat_doc_vectors\n",
    "low_dim_rep = umap.UMAP(\n",
    "    metric=\"cosine\", n_components=5, min_dist=1e-4, random_state=42, n_epochs=500\n",
    ").fit_transform(doc_vectors)\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=25, cluster_selection_method=\"leaf\").fit(\n",
    "    low_dim_rep\n",
    ")\n",
    "tree = clusterer.condensed_tree_.to_pandas()\n",
    "max_lambda = tree.lambda_val[tree.child_size > 1].max()\n",
    "min_lambda = tree.lambda_val[tree.child_size > 1].min()\n",
    "layers = [\n",
    "    clusters_at_level(tree, level)\n",
    "    for level in np.linspace(min_lambda, max_lambda, 9, endpoint=True)[1:-1]\n",
    "]\n",
    "epsilons = [\n",
    "    clusters_eps_at_level(tree, level)\n",
    "    for level in np.linspace(min_lambda, max_lambda, 9, endpoint=True)[1:-1]\n",
    "]\n",
    "leaf_color_key, leaf_dict, leaf_nbrs, embedding_rep = create_leaf_color_key(\n",
    "    clusterer, doc_vectors\n",
    ")\n",
    "\n",
    "\n",
    "layer_plot_elements = []\n",
    "scatterplot_sources = []\n",
    "plot = Figure(title=\"20-Newsgroups Topic Map Explorer\", plot_width=800, plot_height=800)\n",
    "lasso_selector = LassoSelectTool()\n",
    "plot.add_tools(lasso_selector)\n",
    "plot.add_tools(TapTool())\n",
    "plot.add_tools(BoxSelectTool())\n",
    "\n",
    "for layer_idx, layer in enumerate(layers):\n",
    "\n",
    "    cluster_labels, color_key = create_cluster_layer_color_key(\n",
    "        tree, layer, embedding_rep, leaf_nbrs, leaf_color_key, leaf_dict\n",
    "    )\n",
    "    layer_renderers, scatter_source = topic_word_by_cluster_layer(\n",
    "        plot,\n",
    "        layer_idx,\n",
    "        lat_doc_vectors,\n",
    "        lat_word_vectors,\n",
    "        cluster_labels,\n",
    "        epsilons[layer_idx],\n",
    "        embedding_rep,\n",
    "        lambda x: word_vectorizer.token_index_dictionary_[x],\n",
    "        color_key,\n",
    "        n_neighbors=int(2000 / (1 + layer_idx)),\n",
    "    )\n",
    "    layer_plot_elements.append(layer_renderers)\n",
    "    scatterplot_sources.append(scatter_source)\n",
    "\n",
    "for layer_elements in layer_plot_elements[1:]:\n",
    "    for element in layer_elements:\n",
    "        element.visible = False\n",
    "\n",
    "document_source = ColumnDataSource(\n",
    "    dict(document=news_data, newsgroup=[news.target_names[x] for x in targets])\n",
    ")\n",
    "div_of_text = Div(\n",
    "    text=\"<h3 style='color:#2F2F2F;text-align:center;padding:150px 0px;'>up to 100 selected posts display here</h3>\",\n",
    "    width=800,\n",
    "    height=600,\n",
    "    style={\"overflow-y\": \"scroll\", \"height\": \"350px\", \"width\": \"780px\"},\n",
    ")\n",
    "\n",
    "slider_callback = CustomJS(\n",
    "    args=dict(layers=layer_plot_elements),\n",
    "    code=\"\"\"\n",
    "        var selected_layer = cb_obj.value;\n",
    "        for (var i = 0; i < layers.length; i++) {\n",
    "            for (var j = 0; j < layers[i].length; j++) {\n",
    "                if (selected_layer - 1 == i) {\n",
    "                    layers[i][j].visible = true;\n",
    "                } else {\n",
    "                    layers[i][j].visible = false;\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\"\"\",\n",
    ")\n",
    "selection_callback_div = CustomJS(\n",
    "    args=dict(document_source=document_source, div=div_of_text),\n",
    "    code=\"\"\"\n",
    "        var inds = cb_obj.indices;\n",
    "        var d1 = document_source.data;\n",
    "        div.text = \"\";\n",
    "        for (var i = 0; i < inds.length && i < 100; i++) {\n",
    "            div.text += \"<h3 style='text-align:center;color:#2F2F2F;''>\" + d1['newsgroup'][inds[i]] + \"</h3>\";\n",
    "            div.text += \"<pre style='color:#444444;background-color:#dddddd;'>\" + d1['document'][inds[i]] + \"</pre><p/>\";\n",
    "        }\n",
    "        div.change.emit();\n",
    "    \"\"\",\n",
    ")\n",
    "for scatter_source in scatterplot_sources:\n",
    "    scatter_source.selected.js_on_change(\"indices\", selection_callback_div)\n",
    "\n",
    "plot.title.text = \"20-Newsgroups Topic Map Explorer\"\n",
    "plot.title.text_font_size = \"26px\"\n",
    "plot.title.align = \"center\"\n",
    "plot.title.text_color = \"#3F3F3F\"\n",
    "\n",
    "download_callback = CustomJS(\n",
    "    args=dict(document_source=document_source, scatter_sources=scatterplot_sources),\n",
    "    code=\"\"\"\n",
    "function download(filename, content) {\n",
    "  var element = document.createElement('a');\n",
    "  element.setAttribute('href', 'data:text/plain;charset=utf-8,' + encodeURIComponent(content));\n",
    "  element.setAttribute('download', filename);\n",
    "\n",
    "  element.style.display = 'none';\n",
    "  document.body.appendChild(element);\n",
    "\n",
    "  element.click();\n",
    "\n",
    "  document.body.removeChild(element);\n",
    "}\n",
    "\n",
    "// Find selection; get content\n",
    "var csv_content = \"\";\n",
    "var docs = document_source.data;\n",
    "for (var i = 0; i < scatter_sources.length; i++) {\n",
    "    var sel_inds = scatter_sources[i].selected.indices;\n",
    "    for (var j = 0; j < sel_inds.length; j++) {\n",
    "        var ind = sel_inds[j];\n",
    "        var doc_content = docs['document'][ind].replace(/\\\\n/g, \"\\\\\\\\n\").replace(/\"/g, \"'\")\n",
    "        csv_content += ind.toString() + \",\" + docs['newsgroup'][ind] + ',\"' + doc_content + '\"\\\\n';\n",
    "    }\n",
    "}\n",
    "\n",
    "// Start file download.\n",
    "download(\"selected_posts.csv\", csv_content);\n",
    "\"\"\",\n",
    ")\n",
    "\n",
    "layer_slider = Slider(\n",
    "    start=1,\n",
    "    end=len(layers),\n",
    "    value=1,\n",
    "    step=1,\n",
    "    title=\"Cluster Layer (deeper layers have finer clustering)\",\n",
    ")\n",
    "layer_slider.js_on_change(\"value\", slider_callback)\n",
    "download_button = Button(label=\"Download selected posts\", button_type=\"success\")\n",
    "download_button.js_on_click(download_callback)\n",
    "layout = column(plot, layer_slider, div_of_text, download_button)\n",
    "show(layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:docmap]",
   "language": "python",
   "name": "conda-env-docmap-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
